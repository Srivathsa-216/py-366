{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a00e293",
   "metadata": {},
   "source": [
    "### PySpark Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4fae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55269ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b8dff5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kris</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adarsh</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name  Age  Experience\n",
       "0    Shri   29           2\n",
       "1    Kris   22           3\n",
       "2  Adarsh   22           4"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "16f8d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1aecc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5acac0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-KAQQLB1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b399c62a50>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1e0f662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pySpark = spark.read.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bbd131e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff2de220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|   _c0|_c1|       _c2|\n",
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "|  Shri| 29|         2|\n",
      "|  Kris| 22|         3|\n",
      "|Adarsh| 22|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8639e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pySpark = spark.read.option('header', 'true').csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4d675fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|  Shri| 29|         2|\n",
      "|  Kris| 22|         3|\n",
      "|Adarsh| 22|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2003f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bbc2ddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='Shri', Age='29', Experience='2')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cecf68c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Name: string, Age: string, Experience: string]>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Schema\n",
    "df_pySpark.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7499d8",
   "metadata": {},
   "source": [
    "### Pyspark DataFrames Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a23b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here infer schema is used for the understanding of Data types becuase as you can observe above that \n",
    "# by default for all columns it was taking as String as datatype \n",
    "df_pySpark = spark.read.option('header', 'true').csv('data.csv', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e21e8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pySpark = spark.read.csv('data.csv', header = True, inferSchema = True) # The other way of specifying the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "215acd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Name: string, Age: int, Experience: int]>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark.printSchema # As you can see now the age and experience as the type int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fad05c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "90792ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a17a484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Selecting a particular column\n",
    "\n",
    "df_pySpark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6fe6d853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Name|\n",
      "+------+\n",
      "|  Shri|\n",
      "|  Kris|\n",
      "|Adarsh|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "702c2c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting multiple columns\n",
    "\n",
    "df_pySpark.select(['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4032a66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|  Shri| 29|\n",
      "|  Kris| 22|\n",
      "|Adarsh| 22|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "87838ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "68854c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pySpark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "232f20a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+----------+\n",
      "|summary|  Name|               Age|Experience|\n",
      "+-------+------+------------------+----------+\n",
      "|  count|     3|                 3|         3|\n",
      "|   mean|  NULL|24.333333333333332|       3.0|\n",
      "| stddev|  NULL| 4.041451884327381|       1.0|\n",
      "|    min|Adarsh|                22|         2|\n",
      "|    max|  Shri|                29|         4|\n",
      "+-------+------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a95c5951",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pySpark = df_pySpark.withColumn('Experience After 2 Years', df_pySpark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1a104e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------------------------+\n",
      "|  Name|Age|Experience|Experience After 2 Years|\n",
      "+------+---+----------+------------------------+\n",
      "|  Shri| 29|         2|                       4|\n",
      "|  Kris| 22|         3|                       5|\n",
      "|Adarsh| 22|         4|                       6|\n",
      "+------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "632fe7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------------------------+\n",
      "|  Name|Age|Experience|Experience After 2 Years|\n",
      "+------+---+----------+------------------------+\n",
      "|  Shri| 29|         2|                       4|\n",
      "|  Kris| 22|         3|                       5|\n",
      "|Adarsh| 22|         4|                       6|\n",
      "+------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# post reassigning the variable \n",
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cadf69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pySpark = df_pySpark.drop('Experience After 2 Years') # Deleting a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "784d0fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+\n",
      "|  Name|Age|Experience|\n",
      "+------+---+----------+\n",
      "|  Shri| 29|         2|\n",
      "|  Kris| 22|         3|\n",
      "|Adarsh| 22|         4|\n",
      "+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4bccc471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|Username|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|    Shri| 29|         2|\n",
      "|    Kris| 22|         3|\n",
      "|  Adarsh| 22|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pySpark.withColumnRenamed('Name','Username').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4ef29",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b20a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adb8ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7e7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88fdcf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|  Shri|  29|         2|  NULL|\n",
      "|  Kris|  22|         3| 12000|\n",
      "|Adarsh|  22|         4| 81000|\n",
      "| Anand|NULL|        12| 10101|\n",
      "|  NULL|  43|      NULL| 12243|\n",
      "|  NULL|  21|         4|  8425|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bba30e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca0e00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5ad653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NAN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51202eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|  Kris| 22|         3| 12000|\n",
      "|Adarsh| 22|         4| 81000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f20135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6299453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|  Shri|  29|         2|  NULL|\n",
      "|  Kris|  22|         3| 12000|\n",
      "|Adarsh|  22|         4| 81000|\n",
      "| Anand|NULL|        12| 10101|\n",
      "|  NULL|  43|      NULL| 12243|\n",
      "|  NULL|  21|         4|  8425|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95af4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|  Kris| 22|         3| 12000|\n",
      "|Adarsh| 22|         4| 81000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = \"any\").show() # Here it deletes a row if even one value is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3132deb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|  Shri|  29|         2|  NULL|\n",
      "|  Kris|  22|         3| 12000|\n",
      "|Adarsh|  22|         4| 81000|\n",
      "| Anand|NULL|        12| 10101|\n",
      "|  NULL|  43|      NULL| 12243|\n",
      "|  NULL|  21|         4|  8425|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how = \"all\").show() # Here it wont deletes a row even if a value is non null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959009e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|  Shri|  29|         2|  NULL|\n",
      "|  Kris|  22|         3| 12000|\n",
      "|Adarsh|  22|         4| 81000|\n",
      "| Anand|NULL|        12| 10101|\n",
      "|  NULL|  21|         4|  8425|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Threshold values \n",
    "# Means setting a value to be checked for non value like\n",
    "# Atleast 'n' number of values should be non null\n",
    "\n",
    "df.na.drop(how = \"any\", thresh = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32572b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|  Shri| 29|         2|  NULL|\n",
      "|  Kris| 22|         3| 12000|\n",
      "|Adarsh| 22|         4| 81000|\n",
      "|  NULL| 21|         4|  8425|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Subsetting\n",
    "# dropping the rows which are null in the specified column of subset\n",
    "df.na.drop(how = \"any\", subset = ['Age', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617a42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+\n",
      "|  Name| Age|Experience|Salary|\n",
      "+------+----+----------+------+\n",
      "|  Shri|  29|         2|  NULL|\n",
      "|  Kris|  22|         3| 12000|\n",
      "|Adarsh|  22|         4| 81000|\n",
      "| Anand|NULL|        12| 10101|\n",
      "|  NULL|  43|      NULL| 12243|\n",
      "|  NULL|  21|         4|  8425|\n",
      "+------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filling the Missing Values\n",
    "# you can fill the values of na by following method like it takes\n",
    "# 2 paramets i.e.,(value, subset (optional - of type list))\n",
    "\n",
    "df.na.fill(\"Missing values\", [ 'Experience', 'Age' ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "743d241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = ['Age', 'Experience', 'Salary'],\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in ['Age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"mode\")\n",
    "\n",
    "\n",
    "# Similarly you can try it for mode and median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb706e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "|  Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "|  Shri|  29|         2|  NULL|         29|                 2|          8425|\n",
      "|  Kris|  22|         3| 12000|         22|                 3|         12000|\n",
      "|Adarsh|  22|         4| 81000|         22|                 4|         81000|\n",
      "| Anand|NULL|        12| 10101|         22|                12|         10101|\n",
      "|  NULL|  43|      NULL| 12243|         43|                 4|         12243|\n",
      "|  NULL|  21|         4|  8425|         21|                 4|          8425|\n",
      "+------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "\n",
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbac65",
   "metadata": {},
   "source": [
    "### Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d64b45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a382045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|  Shri| 29|         2| 12411|\n",
      "|  Kris| 22|         3| 12000|\n",
      "|Adarsh| 22|         4| 81000|\n",
      "| Anand| 44|        12| 10101|\n",
      "| Karan| 43|         2| 12243|\n",
      "| Kunal| 21|         4|  8425|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37dd74fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Kunal| 21|         4|  8425|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### selecting/filtering the people who has less or equal to 10K salary\n",
    "\n",
    "df.filter(\"Salary <= 10000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa142cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Kunal| 21|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Salary <= 10000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe8a2dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Kunal| 21|         4|  8425|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"Salary\"] <= 10000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39258fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|Age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "| Shri| 29|         2| 12411|\n",
      "| Kris| 22|         3| 12000|\n",
      "|Anand| 44|        12| 10101|\n",
      "|Karan| 43|         2| 12243|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df[\"Salary\"] >= 10000) & \n",
    "        (df[\"Salary\"] <= 30000) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00b3c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+------+\n",
      "|  Name|Age|Experience|Salary|\n",
      "+------+---+----------+------+\n",
      "|Adarsh| 22|         4| 81000|\n",
      "+------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df[\"Salary\"] <= 20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289abea",
   "metadata": {},
   "source": [
    "### GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a5f3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data2.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85eabd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Department: string, Salary: int]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bcfefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------+\n",
      "|     Name| Department|Salary|\n",
      "+---------+-----------+------+\n",
      "|   Adarsh|       Java| 12000|\n",
      "|   Adarsh|DataScience|250000|\n",
      "|   Adarsh| SpringBoot| 50000|\n",
      "|  Krishna|     Devops|125000|\n",
      "|  Krishna|     Python| 60000|\n",
      "|Shrinidhi|       Java|  8000|\n",
      "|Shrinidhi|     Devops| 95000|\n",
      "|     Maya|     Kotlin| 34000|\n",
      "|    Sunil|DataScience| 12200|\n",
      "|    Sunil|     Python| 23000|\n",
      "+---------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dae9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Name: string, Department: string, Salary: int]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98ba0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(Salary)|\n",
      "+---------+-----------+\n",
      "|  Krishna|     185000|\n",
      "|    Sunil|      35200|\n",
      "|   Adarsh|     312000|\n",
      "|Shrinidhi|     103000|\n",
      "|     Maya|      34000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').sum().show() # Grouped to see the total salary of each person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3735e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "| SpringBoot|    50000.0|\n",
      "|     Kotlin|    34000.0|\n",
      "|     Devops|   110000.0|\n",
      "|     Python|    41500.0|\n",
      "|       Java|    10000.0|\n",
      "|DataScience|   131100.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1cf36ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| Department|count|\n",
      "+-----------+-----+\n",
      "| SpringBoot|    1|\n",
      "|     Kotlin|    1|\n",
      "|     Devops|    2|\n",
      "|     Python|    2|\n",
      "|       Java|    2|\n",
      "|DataScience|    2|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f068569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(Salary)|\n",
      "+---------+-----------+\n",
      "|  Krishna|     125000|\n",
      "|    Sunil|      23000|\n",
      "|   Adarsh|     250000|\n",
      "|Shrinidhi|      95000|\n",
      "|     Maya|      34000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Name').max().show() # Each person's maximum salary according to names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5afd4abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     669200|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Salary':'Sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859f1de",
   "metadata": {},
   "source": [
    "### Introduction to Pyspark MLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cca3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e4f71042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-KAQQLB1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f7659420d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6cacb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+\n",
      "|  Name|Age|Experience| Salary|\n",
      "+------+---+----------+-------+\n",
      "|  Shri| 29|         2|  12411|\n",
      "|  Kris| 22|         3|  12000|\n",
      "|Adarsh| 22|         4|  81000|\n",
      "| Anand| 44|        12|  10101|\n",
      "| Karan| 43|         2|  12243|\n",
      "| Kunal| 21|         4|   8425|\n",
      "|  Shri| 29|         2|   1211|\n",
      "|  Kris| 22|         3|  12000|\n",
      "|Adarsh| 22|         4| 231000|\n",
      "| Anand| 44|        12|  10101|\n",
      "| Karan| 43|         2|  12243|\n",
      "| Kunal| 21|         4|   8425|\n",
      "|  Shri| 29|         2|1241211|\n",
      "|  Kris| 22|         3|  12000|\n",
      "|Adarsh| 22|         4|   8100|\n",
      "| Anand| 44|        12|  10101|\n",
      "| Karan| 43|         2|1224123|\n",
      "| Kunal| 21|         4|   8425|\n",
      "|  Shri| 29|         2| 124114|\n",
      "|  Kris| 22|         3|  12000|\n",
      "+------+---+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.read.csv('data.csv', header = True, inferSchema = True)\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "547e6583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bd88488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e085c9",
   "metadata": {},
   "source": [
    "Here we will be making the Age and Expereince column as one independent feature by grouping them with the help\n",
    "of VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb5589dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureAssembler = VectorAssembler(\n",
    "                        inputCols = [\"Age\", \"Experience\"],\n",
    "                        outputCol = \"Independent Features\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0f5297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureAssembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "52dbc3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----------+-------+--------------------+\n",
      "|  Name|Age|Experience| Salary|Independent Features|\n",
      "+------+---+----------+-------+--------------------+\n",
      "|  Shri| 29|         2|  12411|          [29.0,2.0]|\n",
      "|  Kris| 22|         3|  12000|          [22.0,3.0]|\n",
      "|Adarsh| 22|         4|  81000|          [22.0,4.0]|\n",
      "| Anand| 44|        12|  10101|         [44.0,12.0]|\n",
      "| Karan| 43|         2|  12243|          [43.0,2.0]|\n",
      "| Kunal| 21|         4|   8425|          [21.0,4.0]|\n",
      "|  Shri| 29|         2|   1211|          [29.0,2.0]|\n",
      "|  Kris| 22|         3|  12000|          [22.0,3.0]|\n",
      "|Adarsh| 22|         4| 231000|          [22.0,4.0]|\n",
      "| Anand| 44|        12|  10101|         [44.0,12.0]|\n",
      "| Karan| 43|         2|  12243|          [43.0,2.0]|\n",
      "| Kunal| 21|         4|   8425|          [21.0,4.0]|\n",
      "|  Shri| 29|         2|1241211|          [29.0,2.0]|\n",
      "|  Kris| 22|         3|  12000|          [22.0,3.0]|\n",
      "|Adarsh| 22|         4|   8100|          [22.0,4.0]|\n",
      "| Anand| 44|        12|  10101|         [44.0,12.0]|\n",
      "| Karan| 43|         2|1224123|          [43.0,2.0]|\n",
      "| Kunal| 21|         4|   8425|          [21.0,4.0]|\n",
      "|  Shri| 29|         2| 124114|          [29.0,2.0]|\n",
      "|  Kris| 22|         3|  12000|          [22.0,3.0]|\n",
      "+------+---+----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53befbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'Independent Features']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b5d2aa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|Independent Features| Salary|\n",
      "+--------------------+-------+\n",
      "|          [29.0,2.0]|  12411|\n",
      "|          [22.0,3.0]|  12000|\n",
      "|          [22.0,4.0]|  81000|\n",
      "|         [44.0,12.0]|  10101|\n",
      "|          [43.0,2.0]|  12243|\n",
      "|          [21.0,4.0]|   8425|\n",
      "|          [29.0,2.0]|   1211|\n",
      "|          [22.0,3.0]|  12000|\n",
      "|          [22.0,4.0]| 231000|\n",
      "|         [44.0,12.0]|  10101|\n",
      "|          [43.0,2.0]|  12243|\n",
      "|          [21.0,4.0]|   8425|\n",
      "|          [29.0,2.0]|1241211|\n",
      "|          [22.0,3.0]|  12000|\n",
      "|          [22.0,4.0]|   8100|\n",
      "|         [44.0,12.0]|  10101|\n",
      "|          [43.0,2.0]|1224123|\n",
      "|          [21.0,4.0]|   8425|\n",
      "|          [29.0,2.0]| 124114|\n",
      "|          [22.0,3.0]|  12000|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalised_data = output.select(\"Independent Features\", \"Salary\")\n",
    "finalised_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d394434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_set, test_set = finalised_data.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c90a08d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_a3aaa5a62ef5, numFeatures=2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = LinearRegression(featuresCol = \"Independent Features\", labelCol = 'Salary')\n",
    "regressor = regressor.fit(train_set)\n",
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73e0d011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-73310.8865, 74707.2725])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e2b6ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2595041.121001745"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da9f36b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_results = regressor.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31ac1a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-------------------+\n",
      "|Independent Features| Salary|         prediction|\n",
      "+--------------------+-------+-------------------+\n",
      "|          [21.0,4.0]|   8425|  1354341.593984179|\n",
      "|          [21.0,4.0]|   8425|  1354341.593984179|\n",
      "|          [21.0,4.0]|   8425|  1354341.593984179|\n",
      "|          [22.0,3.0]|  12000| 1206323.4349775787|\n",
      "|          [22.0,4.0]|   8100| 1281030.7074624202|\n",
      "|          [22.0,4.0]| 231000| 1281030.7074624202|\n",
      "|          [43.0,2.0]|  12243|-407912.45446419436|\n",
      "|          [43.0,2.0]|1224123|-407912.45446419436|\n",
      "|         [44.0,12.0]|  10101| 265849.38386246096|\n",
      "|         [44.0,12.0]|  10101| 265849.38386246096|\n",
      "+--------------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a23b4810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011872.230850827, 1255468477607.8054)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pred_results.meanAbsoluteError, Pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
