{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e97a134",
   "metadata": {},
   "source": [
    "#  <center><b><i> Basic Snippets related to Data Engineering </i></b></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5170a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abacc84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e999bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Builder',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activeSession',\n",
       " '_conf',\n",
       " '_convert_from_pandas',\n",
       " '_createFromLocal',\n",
       " '_createFromRDD',\n",
       " '_create_dataframe',\n",
       " '_create_from_pandas_with_arrow',\n",
       " '_create_shell_session',\n",
       " '_getActiveSessionOrCreate',\n",
       " '_get_numpy_record_dtype',\n",
       " '_inferSchema',\n",
       " '_inferSchemaFromList',\n",
       " '_instantiatedSession',\n",
       " '_jconf',\n",
       " '_jsc',\n",
       " '_jsparkSession',\n",
       " '_jvm',\n",
       " '_repr_html_',\n",
       " '_sc',\n",
       " 'active',\n",
       " 'addArtifact',\n",
       " 'addArtifacts',\n",
       " 'addTag',\n",
       " 'builder',\n",
       " 'catalog',\n",
       " 'clearTags',\n",
       " 'client',\n",
       " 'conf',\n",
       " 'copyFromLocalToFs',\n",
       " 'createDataFrame',\n",
       " 'getActiveSession',\n",
       " 'getTags',\n",
       " 'interruptAll',\n",
       " 'interruptOperation',\n",
       " 'interruptTag',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'removeTag',\n",
       " 'sparkContext',\n",
       " 'sql',\n",
       " 'stop',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'udf',\n",
       " 'udtf',\n",
       " 'version']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73d7ba",
   "metadata": {},
   "source": [
    "### 1. Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "967c076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createDataFrame in module pyspark.sql.session:\n",
      "\n",
      "createDataFrame(data: Union[pyspark.rdd.RDD[Any], Iterable[Any], ForwardRef('PandasDataFrameLike'), ForwardRef('ArrayLike')], schema: Union[pyspark.sql.types.AtomicType, pyspark.sql.types.StructType, str, NoneType] = None, samplingRatio: Optional[float] = None, verifySchema: bool = True) -> pyspark.sql.dataframe.DataFrame method of pyspark.sql.session.SparkSession instance\n",
      "    Creates a :class:`DataFrame` from an :class:`RDD`, a list, a :class:`pandas.DataFrame`\n",
      "    or a :class:`numpy.ndarray`.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : :class:`RDD` or iterable\n",
      "        an RDD of any kind of SQL data representation (:class:`Row`,\n",
      "        :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`,\n",
      "        :class:`pandas.DataFrame` or :class:`numpy.ndarray`.\n",
      "    schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n",
      "        a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      "        column names, default is None. The data type string format equals to\n",
      "        :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      "        omit the ``struct<>``.\n",
      "    \n",
      "        When ``schema`` is a list of column names, the type of each column\n",
      "        will be inferred from ``data``.\n",
      "    \n",
      "        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      "        from ``data``, which should be an RDD of either :class:`Row`,\n",
      "        :class:`namedtuple`, or :class:`dict`.\n",
      "    \n",
      "        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must\n",
      "        match the real data, or an exception will be thrown at runtime. If the given schema is\n",
      "        not :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      "        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be\n",
      "        \"value\". Each record will also be wrapped into a tuple, which can be converted to row\n",
      "        later.\n",
      "    samplingRatio : float, optional\n",
      "        the sample ratio of rows used for inferring. The first few rows will be used\n",
      "        if ``samplingRatio`` is ``None``.\n",
      "    verifySchema : bool, optional\n",
      "        verify data types of every row against schema. Enabled by default.\n",
      "    \n",
      "        .. versionadded:: 2.1.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Usage with `spark.sql.execution.arrow.pyspark.enabled=True` is experimental.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Create a DataFrame from a list of tuples.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)]).show()\n",
      "    +-----+---+\n",
      "    |   _1| _2|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a list of dictionaries.\n",
      "    \n",
      "    >>> d = [{'name': 'Alice', 'age': 1}]\n",
      "    >>> spark.createDataFrame(d).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  1|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Create a DataFrame with column names specified.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], ['name', 'age']).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the explicit schema specified.\n",
      "    \n",
      "    >>> from pyspark.sql.types import *\n",
      "    >>> schema = StructType([\n",
      "    ...    StructField(\"name\", StringType(), True),\n",
      "    ...    StructField(\"age\", IntegerType(), True)])\n",
      "    >>> spark.createDataFrame([('Alice', 1)], schema).show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame with the schema in DDL formatted string.\n",
      "    \n",
      "    >>> spark.createDataFrame([('Alice', 1)], \"name: string, age: int\").show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create an empty DataFrame.\n",
      "    When initializing an empty DataFrame in PySpark, it's mandatory to specify its schema,\n",
      "    as the DataFrame lacks data from which the schema can be inferred.\n",
      "    \n",
      "    >>> spark.createDataFrame([], \"name: string, age: int\").show()\n",
      "    +----+---+\n",
      "    |name|age|\n",
      "    +----+---+\n",
      "    +----+---+\n",
      "    \n",
      "    Create a DataFrame from Row objects.\n",
      "    \n",
      "    >>> from pyspark.sql import Row\n",
      "    >>> Person = Row('name', 'age')\n",
      "    >>> df = spark.createDataFrame([Person(\"Alice\", 1)])\n",
      "    >>> df.show()\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    \n",
      "    Create a DataFrame from a pandas DataFrame.\n",
      "    \n",
      "    >>> spark.createDataFrame(df.toPandas()).show()  # doctest: +SKIP\n",
      "    +-----+---+\n",
      "    | name|age|\n",
      "    +-----+---+\n",
      "    |Alice|  1|\n",
      "    +-----+---+\n",
      "    >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      "    +---+---+\n",
      "    |  0|  1|\n",
      "    +---+---+\n",
      "    |  1|  2|\n",
      "    +---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.createDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f7e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ac23ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Id |Name  |\n",
      "+---+------+\n",
      "|1  |Adarsh|\n",
      "|2  |Amruth|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 'Adarsh'), (2, 'Amruth')]\n",
    "cols = StructType([\n",
    "    StructField(name = 'Id', dataType = IntegerType()),\n",
    "    StructField(name = 'Name', dataType = StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data= data, schema= cols)\n",
    "\n",
    "df.show(truncate= False)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db42557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|Id |Name  |\n",
      "+---+------+\n",
      "|1  |Adarsh|\n",
      "|2  |Amruth|\n",
      "+---+------+\n",
      "\n",
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using a list of dictionary as a shortcut\n",
    "\n",
    "data = [\n",
    "    {'Id': 1, 'Name': 'Adarsh'},\n",
    "    {'Id': 2, 'Name': 'Amruth'}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data= data)\n",
    "\n",
    "df.show(truncate = False)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30a0db",
   "metadata": {},
   "source": [
    "### 2. Reading a CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ae5631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| Id|   Name|      City|\n",
      "+---+-------+----------+\n",
      "|  1| Adarsh| Bengaluru|\n",
      "|  2| Amruth|    Mysuru|\n",
      "+---+-------+----------+\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |--  Name: string (nullable = true)\n",
      " |--  City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path= 'Data source/emp1.csv', header= True)\n",
    "df.show()\n",
    "df.printSchema()  ### By default the Data type of all columns will be string since we are reading from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c397e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+\n",
      "| Id|   Name|      City|\n",
      "+---+-------+----------+\n",
      "|  1| Adarsh| Bengaluru|\n",
      "|  2| Amruth|    Mysuru|\n",
      "+---+-------+----------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |--  Name: string (nullable = true)\n",
      " |--  City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(path= 'Data source/emp1.csv', header= True, sep= ',', inferSchema= True)\n",
    "df.show()\n",
    "df.printSchema()  ## now because of infer schema you can see the data type is keenly observed on Id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31874bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| Id|      Name|     City|\n",
      "+---+----------+---------+\n",
      "|  3|   Krishna|   Mandya|\n",
      "|  4| Shrinidhi| Virajpet|\n",
      "+---+----------+---------+\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |--  Name: string (nullable = true)\n",
      " |--  City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option(key='header', value= True).load(path= 'Data source/emp2.csv')\n",
    "df.show()\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66283531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| Id|      Name|      City|\n",
      "+---+----------+----------+\n",
      "|  3|   Krishna|    Mandya|\n",
      "|  4| Shrinidhi|  Virajpet|\n",
      "|  1|    Adarsh| Bengaluru|\n",
      "|  2|    Amruth|    Mysuru|\n",
      "+---+----------+----------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |--  Name: string (nullable = true)\n",
      " |--  City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Reading multiple CSV files\n",
    "\n",
    "df = spark.read.csv(path= ['Data source/emp1.csv', 'Data source/emp2.csv'], header= True, inferSchema= True) ### if the csv files are available\n",
    "### on different or same folder and holds same schema then this holds good.\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6698490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| Id|      Name|      City|\n",
      "+---+----------+----------+\n",
      "|  3|   Krishna|    Mandya|\n",
      "|  4| Shrinidhi|  Virajpet|\n",
      "|  1|    Adarsh| Bengaluru|\n",
      "|  2|    Amruth|    Mysuru|\n",
      "+---+----------+----------+\n",
      "\n",
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(field = 'Id', data_type = IntegerType())\\\n",
    "                    .add(field = 'Name', data_type = StringType())\\\n",
    "                    .add(field = 'City', data_type = StringType())\n",
    "\n",
    "df = spark.read.csv(path= ['Data source/emp1.csv', 'Data source/emp2.csv'], header= True,schema= schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dfc85",
   "metadata": {},
   "source": [
    "### 3. Writting Dataframe into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fbe71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be96027",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8e742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DataFrameWriter in module pyspark.sql.readwriter:\n",
      "\n",
      "class DataFrameWriter(OptionUtils)\n",
      " |  DataFrameWriter(df: 'DataFrame')\n",
      " |  \n",
      " |  Interface used to write a :class:`DataFrame` to external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`DataFrame.write`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  .. versionchanged:: 3.4.0\n",
      " |      Supports Spark Connect.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameWriter\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, df: 'DataFrame')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  bucketBy(self, numBuckets: int, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      " |      Buckets the output by the given columns. If specified,\n",
      " |      the output is laid out on the file system similar to Hive's bucketing scheme,\n",
      " |      but with a different bucket hash function and is not compatible with Hive's bucketing.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numBuckets : int\n",
      " |          the number of buckets to save\n",
      " |      col : str, list or tuple\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Applicable for file-based data sources in combination with\n",
      " |      :py:meth:`DataFrameWriter.saveAsTable`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file in a buckted manner, and read it back.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import input_file_name\n",
      " |      >>> # Write a DataFrame into a Parquet file in a bucketed manner.\n",
      " |      ... _ = spark.sql(\"DROP TABLE IF EXISTS bucketed_table\")\n",
      " |      >>> spark.createDataFrame([\n",
      " |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      " |      ...     schema=[\"age\", \"name\"]\n",
      " |      ... ).write.bucketBy(2, \"name\").mode(\"overwrite\").saveAsTable(\"bucketed_table\")\n",
      " |      >>> # Read the Parquet file as a DataFrame.\n",
      " |      ... spark.read.table(\"bucketed_table\").sort(\"age\").show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      |120|Hyukjin Kwon|\n",
      " |      |140| Haejoon Lee|\n",
      " |      +---+------------+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE bucketed_table\")\n",
      " |  \n",
      " |  csv(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, sep: Optional[str] = None, quote: Optional[str] = None, escape: Optional[str] = None, header: Union[bool, str, NoneType] = None, nullValue: Optional[str] = None, escapeQuotes: Union[bool, str, NoneType] = None, quoteAll: Union[bool, str, NoneType] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, ignoreLeadingWhiteSpace: Union[bool, str, NoneType] = None, ignoreTrailingWhiteSpace: Union[bool, str, NoneType] = None, charToEscapeQuoteEscaping: Optional[str] = None, encoding: Optional[str] = None, emptyValue: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      " |      Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
      " |              exists.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a CSV file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file\n",
      " |      ...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
      " |      ...     df.write.csv(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     spark.read.schema(df.schema).format(\"csv\").option(\n",
      " |      ...         \"nullValue\", \"Hyukjin Kwon\").load(d).show()\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |100|NULL|\n",
      " |      +---+----+\n",
      " |  \n",
      " |  format(self, source: str) -> 'DataFrameWriter'\n",
      " |      Specifies the underlying output data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      source : str\n",
      " |          string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1).write.format('parquet')\n",
      " |      <...readwriter.DataFrameWriter object ...>\n",
      " |      \n",
      " |      Write a DataFrame into a Parquet file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.format('parquet').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  insertInto(self, tableName: str, overwrite: Optional[bool] = None) -> None\n",
      " |      Inserts the content of the :class:`DataFrame` to the specified table.\n",
      " |      \n",
      " |      It requires that the schema of the :class:`DataFrame` is the same as the\n",
      " |      schema of the table.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      overwrite : bool, optional\n",
      " |          If true, overwrites existing data. Disabled by default\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unlike :meth:`DataFrameWriter.saveAsTable`, :meth:`DataFrameWriter.insertInto` ignores\n",
      " |      the column names and just uses position-based resolution.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      " |      >>> df = spark.createDataFrame([\n",
      " |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      " |      ...     schema=[\"age\", \"name\"]\n",
      " |      ... )\n",
      " |      >>> df.write.saveAsTable(\"tblA\")\n",
      " |      \n",
      " |      Insert the data into 'tblA' table but with different column names.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age AS col1\", \"name AS col2\").write.insertInto(\"tblA\")\n",
      " |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      |120|Hyukjin Kwon|\n",
      " |      |120|Hyukjin Kwon|\n",
      " |      |140| Haejoon Lee|\n",
      " |      |140| Haejoon Lee|\n",
      " |      +---+------------+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      " |  \n",
      " |  jdbc(self, url: str, table: str, mode: Optional[str] = None, properties: Optional[Dict[str, str]] = None) -> None\n",
      " |      Saves the content of the :class:`DataFrame` to an external database table via JDBC.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      table : str\n",
      " |          Name of the table in the external database.\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      properties : dict\n",
      " |          a dictionary of JDBC database connection arguments. Normally at\n",
      " |          least properties \"user\" and \"password\" with their corresponding values.\n",
      " |          For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Don't create too many partitions in parallel on a large cluster;\n",
      " |      otherwise Spark might crash your external database systems.\n",
      " |  \n",
      " |  json(self, path: str, mode: Optional[str] = None, compression: Optional[str] = None, dateFormat: Optional[str] = None, timestampFormat: Optional[str] = None, lineSep: Optional[str] = None, encoding: Optional[str] = None, ignoreNullFields: Union[bool, str, NoneType] = None) -> None\n",
      " |      Saves the content of the :class:`DataFrame` in JSON format\n",
      " |      (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      " |      specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.json(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.format(\"json\").load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  mode(self, saveMode: Optional[str]) -> 'DataFrameWriter'\n",
      " |      Specifies the behavior when data or table already exists.\n",
      " |      \n",
      " |      Options include:\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Raise an error when writing to an existing path.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 80, \"name\": \"Xinrong Meng\"}]\n",
      " |      ...     ).write.mode(\"error\").format(\"parquet\").save(d) # doctest: +SKIP\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ...AnalysisException: ...\n",
      " |      \n",
      " |      Write a Parquet file back with various options, and read it back.\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Overwrite the path with a new Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Append another DataFrame into the Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 120, \"name\": \"Takuya Ueshin\"}]\n",
      " |      ...     ).write.mode(\"append\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Append another DataFrame into the Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 140, \"name\": \"Haejoon Lee\"}]\n",
      " |      ...     ).write.mode(\"ignore\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.parquet(d).show()\n",
      " |      +---+-------------+\n",
      " |      |age|         name|\n",
      " |      +---+-------------+\n",
      " |      |120|Takuya Ueshin|\n",
      " |      |100| Hyukjin Kwon|\n",
      " |      +---+-------------+\n",
      " |  \n",
      " |  option(self, key: str, value: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      " |      Adds an output option for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : str\n",
      " |          The key for the option to set.\n",
      " |      value\n",
      " |          The value for the option to set.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameWriter object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' with writing a CSV file.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon'.\n",
      " |      ...     df = spark.createDataFrame([(100, None)], \"age INT, name STRING\")\n",
      " |      ...     df.write.option(\"nullValue\", \"Hyukjin Kwon\").mode(\"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame.\n",
      " |      ...     spark.read.schema(df.schema).format('csv').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  options(self, **options: 'OptionalPrimitiveType') -> 'DataFrameWriter'\n",
      " |      Adds output options for the underlying data source.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **options : dict\n",
      " |          The dictionary of string keys and primitive-type values.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> spark.range(1).write.option(\"key\", \"value\")\n",
      " |      <...readwriter.DataFrameWriter object ...>\n",
      " |      \n",
      " |      Specify the option 'nullValue' and 'header' with writing a CSV file.\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
      " |      >>> schema = StructType([\n",
      " |      ...     StructField(\"age\",IntegerType(),True),\n",
      " |      ...     StructField(\"name\",StringType(),True),\n",
      " |      ... ])\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a CSV file with 'nullValue' option set to 'Hyukjin Kwon',\n",
      " |      ...     # and 'header' option set to `True`.\n",
      " |      ...     df = spark.createDataFrame([(100, None)], schema=schema)\n",
      " |      ...     df.write.options(nullValue=\"Hyukjin Kwon\", header=True).mode(\n",
      " |      ...         \"overwrite\").format(\"csv\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the CSV file as a DataFrame.\n",
      " |      ...     spark.read.option(\"header\", True).format('csv').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  orc(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      " |      Saves the content of the :class:`DataFrame` in ORC format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-orc.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a ORC file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a ORC file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.orc(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.format(\"orc\").load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  parquet(self, path: str, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, compression: Optional[str] = None) -> None\n",
      " |      Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : str or list, optional\n",
      " |          names of partitioning columns\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-parquet.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.parquet(d, mode=\"overwrite\")\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.format(\"parquet\").load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  partitionBy(self, *cols: Union[str, List[str]]) -> 'DataFrameWriter'\n",
      " |      Partitions the output by the given columns on the file system.\n",
      " |      \n",
      " |      If specified, the output is laid out on the file system similar\n",
      " |      to Hive's partitioning scheme.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cols : str or list\n",
      " |          name of columns\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file in a partitioned manner, and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> import os\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a Parquet file in a partitioned manner.\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}, {\"age\": 120, \"name\": \"Ruifeng Zheng\"}]\n",
      " |      ...     ).write.partitionBy(\"name\").mode(\"overwrite\").format(\"parquet\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the Parquet file as a DataFrame.\n",
      " |      ...     spark.read.parquet(d).sort(\"age\").show()\n",
      " |      ...\n",
      " |      ...     # Read one partition as a DataFrame.\n",
      " |      ...     spark.read.parquet(f\"{d}{os.path.sep}name=Hyukjin Kwon\").show()\n",
      " |      +---+-------------+\n",
      " |      |age|         name|\n",
      " |      +---+-------------+\n",
      " |      |100| Hyukjin Kwon|\n",
      " |      |120|Ruifeng Zheng|\n",
      " |      +---+-------------+\n",
      " |      +---+\n",
      " |      |age|\n",
      " |      +---+\n",
      " |      |100|\n",
      " |      +---+\n",
      " |  \n",
      " |  save(self, path: Optional[str] = None, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      " |      Saves the contents of the :class:`DataFrame` to a data source.\n",
      " |      \n",
      " |      The data source is specified by the ``format`` and a set of ``options``.\n",
      " |      If ``format`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, optional\n",
      " |          the path in a Hadoop supported file system\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          specifies the behavior of the save operation when data already exists.\n",
      " |      \n",
      " |          * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      " |          * ``overwrite``: Overwrite existing data.\n",
      " |          * ``ignore``: Silently ignore this operation if data already exists.\n",
      " |          * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      " |      partitionBy : list, optional\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a JSON file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a JSON file\n",
      " |      ...     spark.createDataFrame(\n",
      " |      ...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]\n",
      " |      ...     ).write.mode(\"overwrite\").format(\"json\").save(d)\n",
      " |      ...\n",
      " |      ...     # Read the JSON file as a DataFrame.\n",
      " |      ...     spark.read.format('json').load(d).show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      +---+------------+\n",
      " |  \n",
      " |  saveAsTable(self, name: str, format: Optional[str] = None, mode: Optional[str] = None, partitionBy: Union[str, List[str], NoneType] = None, **options: 'OptionalPrimitiveType') -> None\n",
      " |      Saves the content of the :class:`DataFrame` as the specified table.\n",
      " |      \n",
      " |      In the case the table already exists, behavior of this function depends on the\n",
      " |      save mode, specified by the `mode` function (default to throwing an exception).\n",
      " |      When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\n",
      " |      the same as that of the existing table.\n",
      " |      \n",
      " |      * `append`: Append contents of this :class:`DataFrame` to existing data.\n",
      " |      * `overwrite`: Overwrite existing data.\n",
      " |      * `error` or `errorifexists`: Throw an exception if data already exists.\n",
      " |      * `ignore`: Silently ignore this operation if data already exists.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      When `mode` is `Append`, if there is an existing table, we will use the format and\n",
      " |      options of the existing table. The column order in the schema of the :class:`DataFrame`\n",
      " |      doesn't need to be the same as that of the existing table. Unlike\n",
      " |      :meth:`DataFrameWriter.insertInto`, :meth:`DataFrameWriter.saveAsTable` will use the\n",
      " |      column names to find the correct column positions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          the table name\n",
      " |      format : str, optional\n",
      " |          the format used to save\n",
      " |      mode : str, optional\n",
      " |          one of `append`, `overwrite`, `error`, `errorifexists`, `ignore`             (default: error)\n",
      " |      partitionBy : str or list\n",
      " |          names of partitioning columns\n",
      " |      **options : dict\n",
      " |          all other string options\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Creates a table from a DataFrame, and read it back.\n",
      " |      \n",
      " |      >>> _ = spark.sql(\"DROP TABLE IF EXISTS tblA\")\n",
      " |      >>> spark.createDataFrame([\n",
      " |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      " |      ...     schema=[\"age\", \"name\"]\n",
      " |      ... ).write.saveAsTable(\"tblA\")\n",
      " |      >>> spark.read.table(\"tblA\").sort(\"age\").show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      |120|Hyukjin Kwon|\n",
      " |      |140| Haejoon Lee|\n",
      " |      +---+------------+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE tblA\")\n",
      " |  \n",
      " |  sortBy(self, col: Union[str, List[str], Tuple[str, ...]], *cols: Optional[str]) -> 'DataFrameWriter'\n",
      " |      Sorts the output in each bucket by the given columns on the file system.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      col : str, tuple or list\n",
      " |          a name of a column, or a list of names.\n",
      " |      cols : str\n",
      " |          additional names (optional). If `col` is a list it should be empty.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a Parquet file in a sorted-buckted manner, and read it back.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import input_file_name\n",
      " |      >>> # Write a DataFrame into a Parquet file in a sorted-bucketed manner.\n",
      " |      ... _ = spark.sql(\"DROP TABLE IF EXISTS sorted_bucketed_table\")\n",
      " |      >>> spark.createDataFrame([\n",
      " |      ...     (100, \"Hyukjin Kwon\"), (120, \"Hyukjin Kwon\"), (140, \"Haejoon Lee\")],\n",
      " |      ...     schema=[\"age\", \"name\"]\n",
      " |      ... ).write.bucketBy(1, \"name\").sortBy(\"age\").mode(\n",
      " |      ...     \"overwrite\").saveAsTable(\"sorted_bucketed_table\")\n",
      " |      >>> # Read the Parquet file as a DataFrame.\n",
      " |      ... spark.read.table(\"sorted_bucketed_table\").sort(\"age\").show()\n",
      " |      +---+------------+\n",
      " |      |age|        name|\n",
      " |      +---+------------+\n",
      " |      |100|Hyukjin Kwon|\n",
      " |      |120|Hyukjin Kwon|\n",
      " |      |140| Haejoon Lee|\n",
      " |      +---+------------+\n",
      " |      >>> _ = spark.sql(\"DROP TABLE sorted_bucketed_table\")\n",
      " |  \n",
      " |  text(self, path: str, compression: Optional[str] = None, lineSep: Optional[str] = None) -> None\n",
      " |      Saves the content of the DataFrame in a text file at the specified path.\n",
      " |      The text files will be encoded as UTF-8.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      .. versionchanged:: 3.4.0\n",
      " |          Supports Spark Connect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          the path in any Hadoop supported file system\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      Extra options\n",
      " |          For the extra options, refer to\n",
      " |          `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-text.html#data-source-option>`_\n",
      " |          for the version you use.\n",
      " |      \n",
      " |          .. # noqa\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The DataFrame must have only one column that is of string type.\n",
      " |      Each row becomes a new line in the output file.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Write a DataFrame into a text file and read it back.\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory() as d:\n",
      " |      ...     # Write a DataFrame into a text file\n",
      " |      ...     df = spark.createDataFrame([(\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      " |      ...     df.write.mode(\"overwrite\").text(d)\n",
      " |      ...\n",
      " |      ...     # Read the text file as a DataFrame.\n",
      " |      ...     spark.read.schema(df.schema).format(\"text\").load(d).sort(\"alphabets\").show()\n",
      " |      +---------+\n",
      " |      |alphabets|\n",
      " |      +---------+\n",
      " |      |        a|\n",
      " |      |        b|\n",
      " |      |        c|\n",
      " |      +---------+\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DataFrameWriter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb5e5542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [(1, 'Adarsh'), (2, 'Amruth')]\n",
    "cols = ['Id', 'Name']\n",
    "\n",
    "df = spark.createDataFrame(data= data, schema= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b536d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| Id|  Name|\n",
      "+---+------+\n",
      "|  1|Adarsh|\n",
      "|  2|Amruth|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec9cd6c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData source/NewEmp/\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.write.csv(path = 'Data source/NewEmp/', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd249e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
