{"cells":[{"cell_type":"markdown","source":["---\n","\n","Load libraries\n","\n","---"],"metadata":{"id":"HeWgOD1uD1YP"},"id":"HeWgOD1uD1YP"},{"cell_type":"code","source":["## Load libraries\n","import numpy as np\n","import sympy as sp\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","plt.style.use('dark_background')\n","from keras.datasets import mnist\n","%matplotlib inline"],"metadata":{"id":"FXrh9fPyMtwx","executionInfo":{"status":"ok","timestamp":1704015974689,"user_tz":-330,"elapsed":427,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}}},"id":"FXrh9fPyMtwx","execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Set printing precision\n","\n","---"],"metadata":{"id":"ttFTTWbqD4eQ"},"id":"ttFTTWbqD4eQ"},{"cell_type":"code","source":["np.set_printoptions(precision = 2)"],"metadata":{"id":"P3UMZJowDzo0","executionInfo":{"status":"ok","timestamp":1704015987055,"user_tz":-330,"elapsed":431,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}}},"id":"P3UMZJowDzo0","execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Import tensorflow and check version\n","\n","---"],"metadata":{"id":"ntgpzUVDD6yI"},"id":"ntgpzUVDD6yI"},{"cell_type":"code","execution_count":41,"id":"c46bca87","metadata":{"execution":{"iopub.execute_input":"2023-08-18T20:03:32.947591Z","iopub.status.busy":"2023-08-18T20:03:32.946960Z","iopub.status.idle":"2023-08-18T20:03:35.685762Z","shell.execute_reply":"2023-08-18T20:03:35.684891Z"},"origin_pos":7,"tab":["tensorflow"],"id":"c46bca87","executionInfo":{"status":"ok","timestamp":1704016001784,"user_tz":-330,"elapsed":425,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"id":"VA6rtGqLW1jr","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1704016004172,"user_tz":-330,"elapsed":419,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8c5933d4-a836-41d1-e93b-803e01138ec2"},"id":"VA6rtGqLW1jr","execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.15.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["## Mount Google drive folder if running in Colab\n","if('google.colab' in sys.modules):\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount = True)\n","    DIR = '/content/drive/MyDrive/Colab Notebooks/MAHE/MSIS Coursework/OddSem2023MAHE'\n","    DATA_DIR = DIR + '/Data/'\n","else:\n","    DATA_DIR = 'Data/'"],"metadata":{"id":"0s0mudRDMxGf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704016062224,"user_tz":-330,"elapsed":19276,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"8f7e10c4-5f55-4271-a1c3-3b7db776674e"},"id":"0s0mudRDMxGf","execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Calculating softmax loss and gradient for a toy dataset\n","\n","----"],"metadata":{"id":"bDoLtOu1DIeI"},"id":"bDoLtOu1DIeI"},{"cell_type":"code","source":["# Generate artificial data with 5 samples, 4 features per sample\n","# and 3 output classes\n","num_samples = 5 # number of samples\n","num_features = 4 # number of features (a.k.a. dimensionality)\n","num_labels = 3 # number of output labels\n","# Data matrix (each column = single sample)\n","X = np.random.choice(np.arange(0, 5), size = (num_features, num_samples), replace = True)\n","# Class labels\n","y = np.random.choice([0, 1, 2], size = num_samples, replace = True)\n","# Randomly assign entries of weights matrix\n","W = np.random.choice(np.arange(-4, 4), size = (num_labels, num_features), replace = True)\n","print('X = ')\n","print(X)\n","print('y = ')\n","print(y)\n","print('W = ')\n","print(W)"],"metadata":{"id":"r6UN0aHBDJQ7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704016600371,"user_tz":-330,"elapsed":6,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"0b1c2808-4227-48f6-f0c7-60a2ac720425"},"id":"r6UN0aHBDJQ7","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["X = \n","[[1 0 0 1 0]\n"," [3 0 1 3 2]\n"," [2 0 3 4 2]\n"," [3 3 4 2 4]]\n","y = \n","[0 1 0 2 0]\n","W = \n","[[ 1 -2  2 -1]\n"," [-1 -1  3 -2]\n"," [ 1 -3  3  3]]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Add the bias feature to the data matrix (run this cell only once!)\n","\n","---"],"metadata":{"id":"eRbN8y4cDM-a"},"id":"eRbN8y4cDM-a"},{"cell_type":"code","source":["# Add the bias feature to the data matrix (run this cell only once!)\n","print('X = ')\n","print(X)\n","print('X with bias feature = ')\n","X = np.vstack([X, np.ones((1, num_samples))])\n","print(X)"],"metadata":{"id":"8HEBxBkEDVof","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704017184909,"user_tz":-330,"elapsed":409,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"7e1ca104-527d-457b-a4c1-fb3f0bcf9539"},"id":"8HEBxBkEDVof","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["X = \n","[[1 0 0 1 0]\n"," [3 0 1 3 2]\n"," [2 0 3 4 2]\n"," [3 3 4 2 4]]\n","X with bias feature = \n","[[1. 0. 0. 1. 0.]\n"," [3. 0. 1. 3. 2.]\n"," [2. 0. 3. 4. 2.]\n"," [3. 3. 4. 2. 4.]\n"," [1. 1. 1. 1. 1.]]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Adjust the weight matrix with (possibly random) values added\n","for bias as the last column (run this cell only once!)\n","\n","---"],"metadata":{"id":"szHBFLWXDZkO"},"id":"szHBFLWXDZkO"},{"cell_type":"code","source":["# Adjust the weight matrix with (possibly random) values added\n","# for bias as the last column (run this cell only once!)\n","W = np.hstack([W, np.ones((num_labels, 1))])\n","print(W)"],"metadata":{"id":"BCOz6CJgDezS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704017646340,"user_tz":-330,"elapsed":409,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"e2f55fd5-27b6-4d05-d745-6ed7be820c38"},"id":"BCOz6CJgDezS","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1. -2.  2. -1.  1.]\n"," [-1. -1.  3. -2.  1.]\n"," [ 1. -3.  3.  3.  1.]]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Calculate the raw zcores matrix\n","\n","---"],"metadata":{"id":"7BHFFWGQDhK0"},"id":"7BHFFWGQDhK0"},{"cell_type":"code","source":["Z = np.dot(W, X)\n","print('Z = ')\n","print(Z)"],"metadata":{"id":"Kb_1aZBGDnFE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704018113301,"user_tz":-330,"elapsed":703,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"e7d70023-163a-4881-8fea-9c14a01cb544"},"id":"Kb_1aZBGDnFE","execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Z = \n","[[-3. -2.  1.  2. -3.]\n"," [-3. -5.  1.  5. -3.]\n"," [ 8. 10. 19. 11. 13.]]\n"]}]},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cOGX-ZPGZQo-","executionInfo":{"status":"ok","timestamp":1704018127930,"user_tz":-330,"elapsed":3,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"fc5e51af-bf22-4dc3-be28-521da8707e4f"},"id":"cOGX-ZPGZQo-","execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 1 0 2 0]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Define softmax function\n","\n","---\n"],"metadata":{"id":"fm8ySbbDDo5H"},"id":"fm8ySbbDDo5H"},{"cell_type":"code","source":["# Define softmax function\n","def softmax(Z):\n","  # Convert scores to non-normalized probabilites matrix. Note that for each sample,\n","  # that is in each column, the values don't add up to 1. Also note that the\n","  # output values are typically large or small\n","  Z_exp = np.exp(Z - np.max(Z, axis = 0))\n","  # Normalize probabilities matrix such that the sum across each column is equal to 1.\n","  # Now we have actually probability values for each sample.\n","  return(Z_exp / np.sum(Z_exp, axis = 0))"],"metadata":{"id":"MBsVw-CLDuYb","executionInfo":{"status":"ok","timestamp":1704019842470,"user_tz":-330,"elapsed":488,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}}},"id":"MBsVw-CLDuYb","execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Calculate the probability matrix\n","\n","---"],"metadata":{"id":"G4ZIBTCpEAo5"},"id":"G4ZIBTCpEAo5"},{"cell_type":"code","source":["#  Calculate the probability matrix\n","P = softmax(Z)\n","print(Z)\n","print(P)\n","# Sum in each column of matrix P\n","print(np.sum(P, axis = 0))\n","# Print the correct label for each sample\n","print(y)"],"metadata":{"id":"mote5aNPEIwF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704020048729,"user_tz":-330,"elapsed":425,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"9869d282-d7aa-4e9b-a1ab-1066a9944a94"},"id":"mote5aNPEIwF","execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-3. -2.  1.  2. -3.]\n"," [-3. -5.  1.  5. -3.]\n"," [ 8. 10. 19. 11. 13.]]\n","[[1.67e-05 6.14e-06 1.52e-08 1.23e-04 1.13e-07]\n"," [1.67e-05 3.06e-07 1.52e-08 2.47e-03 1.13e-07]\n"," [1.00e+00 1.00e+00 1.00e+00 9.97e-01 1.00e+00]]\n","[1. 1. 1. 1. 1.]\n","[0 1 0 2 0]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Calculate training loss for all samples.\n","\n","---"],"metadata":{"id":"30-kpa-4ELyk"},"id":"30-kpa-4ELyk"},{"cell_type":"code","source":["loss = -np.log(P[y, np.arange(num_samples)])\n","print('Loss = ')\n","print(loss)\n","# Calculate average training loss\n","loss_data = np.mean(loss)\n","print('Total loss = %f'%(loss_data))"],"metadata":{"id":"2qSjIa_RETTf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704020437005,"user_tz":-330,"elapsed":423,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"622b4044-46d9-48f7-8b4b-4c178c771f1d"},"id":"2qSjIa_RETTf","execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Loss = \n","[1.1e+01 1.5e+01 1.8e+01 2.6e-03 1.6e+01]\n","Total loss = 12.000528\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Calculate regularization loss\n","\n","---\n"],"metadata":{"id":"FJahIQnIEWkg"},"id":"FJahIQnIEWkg"},{"cell_type":"code","source":["print(W)\n","print(W[:, :-1])\n","print(W[:, :-1] * W[:, :-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kf9-AmotmbME","executionInfo":{"status":"ok","timestamp":1704021725372,"user_tz":-330,"elapsed":419,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"9dfb0f5a-72d2-4a2b-90e0-52d040bd660d"},"id":"Kf9-AmotmbME","execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 1. -2.  2. -1.  1.]\n"," [-1. -1.  3. -2.  1.]\n"," [ 1. -3.  3.  3.  1.]]\n","[[ 1. -2.  2. -1.]\n"," [-1. -1.  3. -2.]\n"," [ 1. -3.  3.  3.]]\n","[[1. 4. 4. 1.]\n"," [1. 1. 9. 4.]\n"," [1. 9. 9. 9.]]\n"]}]},{"cell_type":"code","source":["# Regularization loss\n","reg = 0.1 # strength of regularization = 10%\n","loss_reg = np.sum(W[:, :-1] * W[:, :-1])\n","print('Total loss = %f'%(loss_data + reg * loss_reg))"],"metadata":{"id":"wqek2LDXEdms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704021917443,"user_tz":-330,"elapsed":417,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"66b3de40-f0a8-469d-b99b-eaba70ca627f"},"id":"wqek2LDXEdms","execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Total loss = 17.300528\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Calculate the gradient of total loss w.r.t. the weights W\n","\n","---\n"],"metadata":{"id":"botmTI2VEg_4"},"id":"botmTI2VEg_4"},{"cell_type":"code","source":["# Adjust the probability matrix such that 1 is subtracted\n","# from each samples correct category probability.\n","P[y, range(num_samples)] = P[y, range(num_samples)]  - 1\n","\n","# Calculate the gradient of total loss w.r.t. the weights W\n","dW =(1/num_samples)*np.dot(P, X.T) + reg * 2 * np.hstack([W[:, :-1], np.zeros((num_labels, 1))])\n","print(dW)"],"metadata":{"id":"TXQ8n2gMEmsE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704023188910,"user_tz":-330,"elapsed":2,"user":{"displayName":"S N S Acharya","userId":"14786945180387920086"}},"outputId":"282c696a-5a5f-45af-e66b-9d70c526d762"},"id":"TXQ8n2gMEmsE","execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-2.00e-01 -2.80e+00 -2.40e+00 -4.60e+00 -1.20e+00]\n"," [-2.00e-01 -1.99e-01  6.02e-01 -1.60e+00 -4.00e-01]\n"," [ 1.99e-01 -1.58e-03  1.20e+00  3.00e+00  5.99e-01]]\n"]}]},{"cell_type":"markdown","source":["---\n","\n","Apply gradient descent to the toy dataset\n","\n","---\n"],"metadata":{"id":"riIs3abkEv5Z"},"id":"riIs3abkEv5Z"},{"cell_type":"code","source":["alpha = 1e-02 # learning rate\n","tol = 1e-05 # stopping tolerance\n","iter = 0\n","maxiter = 1000\n","\n","while np.linalg.norm(dW) > tol and iter < maxiter:\n","  W = W + alpha * (-dW)\n","  iter = iter+1\n","  print('Iteration = %d, ||gradL(W)|| = %f'%(iter, np.linalg.norm(dW)))"],"metadata":{"id":"aBYtL0-gE26z"},"id":"aBYtL0-gE26z","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","TensorFlow includes a low-level API known as TensorFlow core and many high-level APIs, including Keras (tf.keras).\n","\n","Now we will focus on the TensorFlow low-level API starting with *TensorFlow constants* (https://www.tensorflow.org/guide/tensor) which have the following proprties:\n","\n","1. Values are stored at the time of defining the tensor\n","2. Immutable\n","\n","---"],"metadata":{"id":"NJxvRKDPFESC"},"id":"NJxvRKDPFESC"},{"cell_type":"code","source":["T1 = tf.constant(5.0, dtype = tf.float16, name = 't1')\n","T2 = tf.constant(8.0, dtype = tf.float16, name = 't2')\n","T3 = tf.constant(10.0, dtype = tf.float16, name = 't3')\n","T4 = tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype = tf.float32, name = 't4')\n","\n","print(T1)\n","print(T2)\n","print(T3)\n","print(T4)"],"metadata":{"id":"k8XDUQasFdui"},"id":"k8XDUQasFdui","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Elementwise operations on constant tensors\n","\n","---"],"metadata":{"id":"L7qHsHEnD4vc"},"id":"L7qHsHEnD4vc"},{"cell_type":"code","source":["print(T1+T2)\n","print(T1-T2)"],"metadata":{"id":"Q9XIPhrBD-Tw"},"id":"Q9XIPhrBD-Tw","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Built-in operations\n","\n","---"],"metadata":{"id":"PFHaxm0lEadg"},"id":"PFHaxm0lEadg"},{"cell_type":"code","source":["op1 = tf.add(T1, T2)\n","op2 = tf.exp(T4)\n","print(op1)\n","print(op2)"],"metadata":{"id":"HC0M09CWEdq2"},"id":"HC0M09CWEdq2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","TensorFlow session is applicable only for **TensorFlow version 1** which allows for defining a computation (data flow) graph such that the nodes are the operations and edges are the tensors followed by an execution of the graph.\n","\n","**TensorFlow version 2** has eager execution (execute immediately withut creating a session).\n","\n","If version 1 is to be used then, we import TensorFlow as follows:\n","\n","$$\\begin{align*}&\\texttt{import tensorflow.compat.v1 as tf}\\\\&\\texttt{\n","tf.disable_v2_behavior()}\\end{align*}$$\n","\n","or eager execution in version 2 can be disabled using $$\\begin{align*}&\\texttt{import tensorflow as tf}\\\\&\\texttt{tf.compat.v1.disable_eager_execution()}\\end{align*}$$\n","\n","----"],"metadata":{"id":"o298a8PsGXT_"},"id":"o298a8PsGXT_"},{"cell_type":"code","source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","tf.__version__"],"metadata":{"id":"LR6V-3qxMbkT"},"id":"LR6V-3qxMbkT","execution_count":null,"outputs":[]},{"cell_type":"code","source":["T1 = tf.constant(5.0, dtype = tf.float16, name = 't1')\n","T2 = tf.constant(8.0, dtype = tf.float16, name = 't2')\n","T4 = tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]], dtype = tf.float32, name = 't4')\n","op1 = tf.add(T1, T2)\n","op2 = tf.exp(T4)\n","print(op1)\n","print(op2)\n","with tf.Session() as sess:\n","  print(sess.run(op1))\n","  print(sess.run(op2))"],"metadata":{"id":"Dk9A2mzaGaBJ"},"id":"Dk9A2mzaGaBJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Placeholders (applicable only for **TensorFlow version 1**):  a TensorFlow computation graph can be parameterized to accept external inputs (such as input data for a machine learning algorithm) during runtime using placeholders. That is, placeholders let are empty tensors whose values can be provided at runtime.\n","\n","---\n","\n"],"metadata":{"id":"pMv5QzcMFyqD"},"id":"pMv5QzcMFyqD"},{"cell_type":"code","source":["T1 = tf.placeholder(tf.float32)\n","T2 = tf.placeholder(tf.float32)\n","# Define some operations\n","op1 = T1 + T2\n","op2 = T1 * T2\n","with tf.Session() as sess:\n","  print('mutiply: ', sess.run(op1, feed_dict = {T1: 2, T2: 3}))\n","  print('add: ', sess.run(op2, feed_dict = {T1: 2, T2: 3}))"],"metadata":{"id":"BLY-tdRIF1PG"},"id":"BLY-tdRIF1PG","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Re-import TensorFlow version 2\n","\n","---"],"metadata":{"id":"050j_5J2Nah7"},"id":"050j_5J2Nah7"},{"cell_type":"code","source":["import tensorflow as tf\n","tf.__version__"],"metadata":{"id":"ZOKh7MMnNaGT"},"id":"ZOKh7MMnNaGT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Tensors from other Python objects such as lists, NumPy arrays, and pandas DataFrames using tf.convert_to_tensor()\n","\n","---"],"metadata":{"id":"1BU193KqGNaC"},"id":"1BU193KqGNaC"},{"cell_type":"code","source":["T = tf.convert_to_tensor(np.array([1, 2, 3, 4]), dtype = tf.float64)\n","print(T)"],"metadata":{"id":"cHvb1OOUGRJc"},"id":"cHvb1OOUGRJc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Variables (https://www.tensorflow.org/guide/variable)\n","\n","---"],"metadata":{"id":"xdMtCNxXGRg7"},"id":"xdMtCNxXGRg7"},{"cell_type":"code","source":[],"metadata":{"id":"iXBvDD0SGfOD"},"id":"iXBvDD0SGfOD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","Automatic differentiation using TF (https://www.tensorflow.org/guide/autodiff)\n","\n","Example: calculate the sensitivity of $L(w) = 4w+w^3$ w.r.t. the input $w$ at $w=1.$\n","\n","Sensitivity $\\nabla_wL = 4+3w^2,$ which at $w=1$ is equal to $4+3\\times1^2=7.$\n","\n","---"],"metadata":{"id":"GzFSQ-9VGfhs"},"id":"GzFSQ-9VGfhs"},{"cell_type":"code","source":[],"metadata":{"id":"BPaQg5NaGkQ9"},"id":"BPaQg5NaGkQ9","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"required_libs":[],"colab":{"provenance":[{"file_id":"https://github.com/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_preliminaries/ndarray.ipynb","timestamp":1702528860550}]}},"nbformat":4,"nbformat_minor":5}